# Supplier Risk & Relationship Management (SRRM)

## Overview

The **Supplier Risk & Relationship Management (SRRM)** system is an end-to-end data and ML-driven platform designed to **assess, predict, and explain supplier risk**.
It integrates structured supplier data, machine learning predictions, workflow orchestration, and an interactive frontend dashboard.

The system is built to demonstrate a **real-world enterprise analytics pipeline**, including explainable AI (XAI) using SHAP.

---

## High-Level Project Flow

1. **Supplier & Risk Data Ingestion**

   * Supplier metadata is stored in `supplier_profile`
   * Periodic operational and risk metrics are stored in `supplier_risk_master`
   * Newly added records are marked with `is_predicted = false`

2. **Prediction Pipeline (Apache Airflow)**

   * An Airflow DAG fetches only unpredicted rows
   * A trained **LightGBM model** predicts supplier risk
   * Outputs are written to:

     * `risk_prediction_history` (predictions + probabilities)
     * `shap_explanations` (feature-level explanations)
   * Processed rows are marked as `is_predicted = true`

3. **Explainability**

   * SHAP values explain **why** a supplier was classified as high/medium/low risk
   * Stored as JSON for flexible visualization

4. **Frontend Dashboard**

   * Built using **Streamlit**
   * Allows users to:

     * View all tables
     * Upload new CSV data
     * Trigger the Airflow DAG
     * Analyze predictions and SHAP explanations
     * Identify high-risk suppliers

---

## System Architecture (Logical)

```
CSV / UI Input
      â†“
Supabase (PostgreSQL)
      â†“
Apache Airflow DAG
      â†“
LightGBM + SHAP
      â†“
Predictions + Explanations
      â†“
Streamlit Dashboard
```

---

## Tech Stack

* **Database:** Supabase (PostgreSQL)
* **ML Model:** LightGBM
* **Explainability:** SHAP
* **Orchestration:** Apache Airflow
* **Backend Access:** Supabase Python Client
* **Frontend:** Streamlit
* **Language:** Python 3.10

---

## Repository Structure

```
srrm_system/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ srrm_prediction_dag.py
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ lgbm_model.txt
â”‚       â”œâ”€â”€ preprocessors.pkl
â”‚       â”œâ”€â”€ feature_names.json
â”‚       â””â”€â”€ model_metadata.json
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ airflow_venv/
â”‚
â””â”€â”€ .env
```

---

## Local Setup Instructions

### 1. Prerequisites

* Python **3.10**
* Supabase project (tables already created)
* Airflow 2.8.x
* macOS / Linux recommended

---

### 2. Clone / Create Project Directory

```bash
mkdir srrm_system
cd srrm_system
```

---

### 3. Create Virtual Environment

```bash
python3.10 -m venv airflow_venv
source airflow_venv/bin/activate
```

---

### 4. Install Dependencies

```bash
pip install apache-airflow==2.8.1 \
            lightgbm \
            shap \
            pandas \
            joblib \
            supabase \
            streamlit \
            python-dotenv \
            requests
```

---

### 5. Environment Variables

Create a `.env` file in the project root:

```env
SUPABASE_URL=https://<your-project>.supabase.co
SUPABASE_SERVICE_ROLE_KEY=<your-service-role-key>

AIRFLOW_API=http://localhost:8080/api/v1
AIRFLOW_USER=admin
AIRFLOW_PASS=admin
```

Load them before running:

```bash
export $(cat .env | xargs)
```

---

### 6. Initialize Airflow

```bash
export AIRFLOW_HOME=$(pwd)/airflow
airflow db init
airflow users create \
  --username admin \
  --firstname admin \
  --lastname admin \
  --role Admin \
  --email admin@example.com \
  --password admin
```

---

### 7. Start Airflow

In **two terminals**:

**Terminal 1**

```bash
airflow webserver --port 8080
```

**Terminal 2**

```bash
airflow scheduler
```

Access Airflow UI at:
ðŸ‘‰ [http://localhost:8080](http://localhost:8080)

---

### 8. Place Model Artifacts

Ensure the following files exist:

```
airflow/models/
â”œâ”€â”€ lgbm_model.txt
â”œâ”€â”€ preprocessors.pkl
â”œâ”€â”€ feature_names.json
â”œâ”€â”€ model_metadata.json
```

---

### 9. Run Frontend

```bash
cd frontend
streamlit run app.py
```

Access UI at:
ðŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## How to Use the System

1. Upload supplier risk data via the frontend (CSV)
2. Click **Run Prediction Pipeline**
3. Airflow triggers the DAG
4. Predictions and SHAP explanations are generated
5. View results and high-risk suppliers in the dashboard

---


